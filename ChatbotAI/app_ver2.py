import streamlit as st
import tempfile
import os
import shutil
import re
import pymupdf4llm 
import time

import chromadb 
from chromadb.errors import InternalError as ChromaInternalError

# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
# [ì¶”ê°€] 2ë‹¨ê³„ ì²­í‚¹ì„ ìœ„í•œ ìŠ¤í”Œë¦¬í„°
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

PERSIST_DIRECTORY = "./chroma_db"

st.set_page_config(page_title="ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (v2.0)", layout="wide")
st.title("ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (ì •í™•ë„ ê³ ë„í™” Ver.)")

# --------------------------------------------------------------------------------
# 1. ë¬¸ì„œ ì „ì²˜ë¦¬ ë¡œì§ (2ë‹¨ê³„ ì²­í‚¹ ì ìš©)
# --------------------------------------------------------------------------------
def clean_markdown_text(text):
    text = text.replace("~~", "") 
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text

def process_pdf_to_structured_docs(file_path, source_name):
    # 1. PDF -> Markdown
    md_text = pymupdf4llm.to_markdown(file_path)
    md_text = clean_markdown_text(md_text)
    
    # 2. êµ¬ì¡°í™” (ì œNì¡°, ë³„í‘œ ë“±ì„ í—¤ë”ë¡œ ë³€í™˜)
    md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\1# \2', md_text)
    md_text = re.sub(r'(^|\n)(\[ë³„í‘œ\s*\d+.*?\])', r'\1# \2', md_text)
    md_text = re.sub(r'(^|\n)(\[ë³„ì§€\s*.*?\])', r'\1# \2', md_text)

    # 3. [1ë‹¨ê³„] í—¤ë” ê¸°ë°˜ ë¶„í•  (ì¡°í•­ ë‹¨ìœ„)
    headers_to_split_on = [("#", "Article_Title")]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    header_splits = markdown_splitter.split_text(md_text)
    
    # 4. [2ë‹¨ê³„] ì¬ê·€ì  ë¬¸ì ë¶„í•  (ê¸´ ì¡°í•­ ì„¸ë¶€ ë¶„í• )
    # chunk_size=800: í•œê¸€ ê¸°ì¤€ ë¬¸ë§¥ íŒŒì•…ì— ì ì ˆí•œ ê¸¸ì´
    # chunk_overlap=100: ì˜ë¦° ë¶€ë¶„ì˜ ë¬¸ë§¥ ì—°ê²°ì„ ìœ„í•´ ê²¹ì¹˜ê¸°
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100
    )
    
    final_docs = []
    for doc in header_splits:
        # í—¤ë” ë©”íƒ€ë°ì´í„°(ì¡°í•­ ì œëª©)ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‚´ìš©ì„ ë” ì˜ê²Œ ìª¼ê°­ë‹ˆë‹¤.
        splits = text_splitter.split_text(doc.page_content)
        for split_content in splits:
            new_doc = Document(
                page_content=split_content,
                metadata={
                    "source": source_name,
                    "Article_Title": doc.metadata.get("Article_Title", "ì¼ë°˜"),
                    "category": "table" if "|" in split_content else "text" # í‘œ í¬í•¨ ì—¬ë¶€ ë©”íƒ€ë°ì´í„°
                }
            )
            final_docs.append(new_doc)
        
    return final_docs

# --------------------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” (ì„¤ì •)
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    CUSTOM_MODELS = ["korean-llama3", "korean-gemma2"] 
    selected_model = st.selectbox("AI ëª¨ë¸ ì„ íƒ", CUSTOM_MODELS, index=0)

    st.markdown("---")
    st.header("ğŸ“‚ ê·œì • ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("PDF ê·œì • íŒŒì¼", type=["pdf"], accept_multiple_files=True)
    process_button = st.button("ğŸš€ ê·œì • í•™ìŠµ ì‹œì‘")

    st.markdown("---")
    st.header("ğŸ“š ë“±ë¡ëœ ê·œì • ëª©ë¡")
    if 'learned_files' not in st.session_state:
        st.session_state.learned_files = []
        
    if st.session_state.learned_files:
        for f_name in st.session_state.learned_files:
            st.success(f"â€¢ {f_name}")
    else:
        st.info("ë“±ë¡ëœ ê·œì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” (rm -rf ./chroma_db)"):
        st.session_state.clear() 
        try:
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY)
                if 'learned_files' in st.session_state:
                    del st.session_state.learned_files
                st.success("âœ… DB ì‚­ì œ ì™„ë£Œ! F5ë¥¼ ëˆŒëŸ¬ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
            else:
                st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âš ï¸ íŒŒì¼ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}")

# --------------------------------------------------------------------------------
# 3. ì„ë² ë”© ë° DB ì²˜ë¦¬
# --------------------------------------------------------------------------------
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

if uploaded_files and process_button:
    with st.spinner("2ë‹¨ê³„ ì²­í‚¹(Header+Recursive) ë° í•™ìŠµ ì§„í–‰ ì¤‘..."):
        all_docs = []
        for file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(file.getvalue())
                tmp_path = tmp.name
            
            try:
                docs = process_pdf_to_structured_docs(tmp_path, file.name)
                all_docs.extend(docs)
            finally:
                os.remove(tmp_path)
        
        if all_docs:
            vectorstore = Chroma(
                persist_directory=PERSIST_DIRECTORY,
                embedding_function=get_embeddings()
            )
            vectorstore.add_documents(all_docs)
            st.success(f"âœ… ì´ {len(all_docs)}ê°œì˜ ì²­í¬ê°€ ì •ë°€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            for file in uploaded_files:
                if file.name not in st.session_state.learned_files:
                    st.session_state.learned_files.append(file.name)
            
            time.sleep(1)
            st.rerun()

# --------------------------------------------------------------------------------
# 4. ê²€ìƒ‰ ë° ë‹µë³€ ë¡œì§ (Advanced Retrieval)
# --------------------------------------------------------------------------------
embeddings = get_embeddings()
vectorstore = None
ensemble_retriever = None

if os.path.exists(PERSIST_DIRECTORY):
    try:
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
        
        # [íŠœë‹ 1] ê²€ìƒ‰ í›„ë³´(k) ì¦ê°€: BM25ì™€ Chroma ëª¨ë‘ 10ê°œì”© ê²€ìƒ‰
        chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
        
        doc_data = vectorstore.get()
        if doc_data['documents']:
            bm25_docs = [Document(page_content=t, metadata=m) for t, m in zip(doc_data['documents'], doc_data['metadatas'])]
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 10
            
            # [íŠœë‹ 2] ê°€ì¤‘ì¹˜ ì¡°ì • (0.5 : 0.5) - í‚¤ì›Œë“œì™€ ì˜ë¯¸ ê²€ìƒ‰ì˜ ê· í˜•
            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, chroma_retriever],
                weights=[0.5, 0.5]
            )
        else:
            ensemble_retriever = chroma_retriever
            
    except ChromaInternalError:
        st.error("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì ê¸ˆ ì˜¤ë¥˜: ì„œë²„ ì¬ì‹œì‘ í•„ìš”")
        ensemble_retriever = None
    except Exception as e:
        st.error(f"DB ë¡œë“œ ì˜¤ë¥˜: {e}")
        ensemble_retriever = None

# ì±„íŒ… UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

if prompt := st.chat_input("ê·œì •ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        if ensemble_retriever:
            # [íŠœë‹ 3] ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • (ê²°ì •ë¡ ì  ì¶œë ¥)
            llm = ChatOllama(
                model=selected_model, 
                base_url="http://127.0.0.1:11434",
                temperature=0,   # ì°½ì˜ì„± 0
                top_p=0.1        # í™•ì‹¤í•œ ë‹¨ì–´ë§Œ ì„ íƒ
            )
            
            # [íŠœë‹ 4] ì¤‘ë³µ ë¬¸ì„œ ì œê±° (Post-processing)
            retrieved_docs = ensemble_retriever.invoke(prompt)
            unique_docs = []
            seen_content = set()
            
            for doc in retrieved_docs:
                # ë‚´ìš©ì´ 95% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°
                content_snippet = doc.page_content[:100] 
                if content_snippet not in seen_content:
                    unique_docs.append(doc)
                    seen_content.add(content_snippet)
            
            # ìƒìœ„ 5ê°œë§Œ ìµœì¢…ì ìœ¼ë¡œ LLMì— ì „ë‹¬
            final_context_docs = unique_docs[:5]

            # ë¬¸ë§¥ ì¡°í•©
            context_text = "\n\n".join([doc.page_content for doc in final_context_docs])

            # [íŠœë‹ 5] í”„ë¡¬í”„íŠ¸ ê°•í™” (í”„ë¦¬í•„ ìœ ë„ í¬í•¨)
            template = """
            [System Instruction]
            ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì • ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ [Context]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            
            [ì œì•½ ì¡°ê±´]
            1. ì¶œë ¥ ì–¸ì–´: **í•œêµ­ì–´(Korean)**ë§Œ ì‚¬ìš©.
            2. ê·¼ê±° ì œì‹œ: "ê·œì • ì œOOì¡°ì— ì˜í•˜ë©´..." í˜•ì‹ì„ ì‚¬ìš©í•  ê²ƒ.
            3. í‘œ(Table): ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•  ê²ƒ.
            4. ëª¨ë¦„: ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ê·œì •ì— í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•  ê²ƒ.

            [Context]:
            {context}

            [Question]:
            {question}

            ë‹µë³€(í•œêµ­ì–´):
            """
            
            # LangChain PromptTemplate
            prompt = PromptTemplate(
                input_variables=["context", "question"],
                template=template
            )

            # Chain ì‹¤í–‰ (ìˆ˜ë™ êµ¬ì„±)
            formatted_prompt = prompt.format(context=context_text, question=prompt)
            
            try:
                with st.spinner("ì •ë°€ ë¶„ì„ ì¤‘..."):
                    # LLM ì§ì ‘ í˜¸ì¶œ
                    response = llm.invoke(formatted_prompt).content
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                with st.expander("ì°¸ê³ í•œ ê·œì • ì²­í¬(Chunks)"):
                    for i, doc in enumerate(final_context_docs):
                         title = doc.metadata.get("Article_Title", "ì¡°í•­/ë³„í‘œ")
                         st.markdown(f"**[ì°¸ê³  {i+1}: {title}]**")
                         st.text(doc.page_content[:200] + "...")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")