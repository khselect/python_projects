import streamlit as st
import tempfile
import os
import shutil
import re
import pymupdf4llm 
import time
import mammoth  # docx ë³€í™˜ìš©
import markdownify # html to markdown ìš©
import olefile # hwp ê¸°ì´ˆ ë¶„ì„ìš©
import sys
print(f"í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì´ì¬ ê²½ë¡œ: {sys.executable}")

# [ìˆ˜ì •] ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¤ë˜ from chromadb.errors ... êµ¬ë¬¸ì„ ì‚­ì œí•˜ê³  ê¸°ë³¸ ëª¨ë“ˆë§Œ import í•©ë‹ˆë‹¤.
import chromadb 

# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_chroma import Chroma

from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter


PERSIST_DIRECTORY = "./chroma_db"

st.set_page_config(page_title="ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (v3.1)", layout="wide")
st.title("ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (Ver.1.0)")

# --------------------------------------------------------------------------------
# 1. ë¬¸ì„œ ì „ì²˜ë¦¬ ë¡œì§ (PDF, DOCX, HWP í†µí•©)
# --------------------------------------------------------------------------------
def clean_markdown_text(text):
    text = text.replace("~~", "") 
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text

def convert_docx_to_markdown(docx_path):
    with open(docx_path, "rb") as docx_file:
        result = mammoth.convert_to_html(docx_file)
        html = result.value
    md_text = markdownify.markdownify(html, heading_style="ATX")
    return md_text

def extract_hwp_text(hwp_path):
    try:
        f = olefile.OleFileIO(hwp_path)
        encoded_text = f.openstream("PrvText").read()
        decoded_text = encoded_text.decode("utf-16le")
        return decoded_text
    except Exception as e:
        return f"[HWP ë³€í™˜ ì˜¤ë¥˜] HWP íŒŒì¼ì€ Word(.docx)ë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\nì˜¤ë¥˜ë‚´ìš©: {e}"

def process_file_to_docs(file, source_name):
    file_ext = os.path.splitext(file.name)[1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(file.getvalue())
        tmp_path = tmp.name

    try:
        if file_ext == ".pdf":
            md_text = pymupdf4llm.to_markdown(tmp_path)
        elif file_ext == ".docx":
            md_text = convert_docx_to_markdown(tmp_path)
        elif file_ext in [".hwp", ".hwpx"]:
            raw_text = extract_hwp_text(tmp_path)
            md_text = clean_markdown_text(raw_text)
            md_text = f"# {source_name} ë³¸ë¬¸\n\n{md_text}"
        else:
            return []

        md_text = clean_markdown_text(md_text)
        
        md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\1# \2', md_text)
        md_text = re.sub(r'(^|\n)(\[ë³„í‘œ\s*\d+.*?\])', r'\1# \2', md_text)
        md_text = re.sub(r'(^|\n)(\[ë³„ì§€\s*.*?\])', r'\1# \2', md_text)

        headers_to_split_on = [("#", "Article_Title")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        header_splits = markdown_splitter.split_text(md_text)
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        
        final_docs = []
        for doc in header_splits:
            splits = text_splitter.split_text(doc.page_content)
            for split_content in splits:
                new_doc = Document(
                    page_content=split_content,
                    metadata={
                        "source": source_name,
                        "Article_Title": doc.metadata.get("Article_Title", "ì¼ë°˜"),
                        "file_type": file_ext
                    }
                )
                final_docs.append(new_doc)
                
        return final_docs
        
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# --------------------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” (ì„¤ì •)
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    CUSTOM_MODELS = ["korean-llama3", "korean-gemma2"] 
    selected_model = st.selectbox("AI ëª¨ë¸ ì„ íƒ", CUSTOM_MODELS, index=0)

    st.markdown("---")
    st.header("ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ê·œì • íŒŒì¼ (PDF, Word ê¶Œì¥)", 
        type=["pdf", "docx", "hwp"], 
        accept_multiple_files=True
    )
    if uploaded_files:
        st.caption("ğŸ’¡ Tip: í‘œê°€ í¬í•¨ëœ ê·œì •ì€ **Word(.docx)** íŒŒì¼ì´ ê°€ì¥ ì •í™•í•©ë‹ˆë‹¤.")

    process_button = st.button("ğŸš€ ê·œì • í•™ìŠµ ì‹œì‘")

    st.markdown("---")
    st.header("ğŸ“š ë“±ë¡ëœ ê·œì • ëª©ë¡")
    if 'learned_files' not in st.session_state:
        st.session_state.learned_files = []
        
    if st.session_state.learned_files:
        for f_name in st.session_state.learned_files:
            st.success(f"â€¢ {f_name}")
    else:
        st.info("ë“±ë¡ëœ ê·œì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” (rm -rf)"):
        st.session_state.clear() 
        try:
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY)
                if 'learned_files' in st.session_state:
                    st.session_state.learned_files = []
                st.success("âœ… DB ì‚­ì œ ì™„ë£Œ! F5ë¥¼ ëˆŒëŸ¬ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
            else:
                st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âš ï¸ íŒŒì¼ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}\n(OneDrive ì‚¬ìš© ì‹œ ë™ê¸°í™”ë¥¼ ì¼ì‹œ ì¤‘ì§€í•˜ê±°ë‚˜ í´ë”ë¥¼ ì¼ë°˜ ê²½ë¡œë¡œ ì˜®ê²¨ì£¼ì„¸ìš”.)")

# --------------------------------------------------------------------------------
# 3. ì„ë² ë”© ë° DB ì²˜ë¦¬
# --------------------------------------------------------------------------------
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

if uploaded_files and process_button:
    with st.spinner("ë‹¤ì–‘í•œ ë¬¸ì„œ í¬ë§· ë³€í™˜ ë° ì •ë°€ í•™ìŠµ ì¤‘..."):
        all_docs = []
        for file in uploaded_files:
            try:
                docs = process_file_to_docs(file, file.name)
                all_docs.extend(docs)
            except Exception as e:
                st.error(f"'{file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if all_docs:
            vectorstore = Chroma(
                persist_directory=PERSIST_DIRECTORY,
                embedding_function=get_embeddings()
            )
            vectorstore.add_documents(all_docs)
            st.success(f"âœ… ì´ {len(all_docs)}ê°œì˜ ì²­í¬ê°€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            for file in uploaded_files:
                if file.name not in st.session_state.learned_files:
                    st.session_state.learned_files.append(file.name)
            
            time.sleep(1)
            st.rerun()

# --------------------------------------------------------------------------------
# 4. ê²€ìƒ‰ ë° ë‹µë³€ ë¡œì§ (ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ì ìš©)
# --------------------------------------------------------------------------------
embeddings = get_embeddings()
vectorstore = None
ensemble_retriever = None

if os.path.exists(PERSIST_DIRECTORY):
    try:
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embeddings
        )

        ensemble_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
        st.success("âœ… ê·œì • DB ë¡œë“œ ì™„ë£Œ")

    except Exception as e:
        st.error(f"âŒ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.warning("âš ï¸ Chroma DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ì±„íŒ… UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

if prompt := st.chat_input("ê·œì •ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        if ensemble_retriever:
            llm = ChatOllama(
                model=selected_model, 
                base_url="http://127.0.0.1:11434",
                temperature=0,
                top_p=0.1
            )
            
            retrieved_docs = ensemble_retriever.invoke(prompt)

            unique_docs = []
            seen_content = set()
            for doc in retrieved_docs:
                content_snippet = doc.page_content[:100] 
                if content_snippet not in seen_content:
                    unique_docs.append(doc)
                    seen_content.add(content_snippet)
            final_context_docs = unique_docs[:5]
            context_text = "\n\n".join([doc.page_content for doc in final_context_docs])

            template = """
            [System Instruction]
            ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì • ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ [Context]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            ì‚¬ìš©ìê°€ ì½ê¸° í¸í•˜ê²Œ ë°”ë¡œ ê²°ë¡ ë¶€í„° ë‹µë³€í•˜ì„¸ìš”. 
            "ì œì•½ ì¡°ê±´ì„ ì¤€ìˆ˜í–ˆìŠµë‹ˆë‹¤" ê°™ì€ ë¶ˆí•„ìš”í•œ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.

            [Context]:
            {context}

            [Question]:
            {question}

            ë‹µë³€(í•œêµ­ì–´):
            """
            
            prompt_obj = PromptTemplate(
                input_variables=["context", "question"],
                template=template
            )
            formatted_prompt = prompt_obj.format(context=context_text, question=prompt)
            
            try:
                with st.spinner("ì •ë°€ ë¶„ì„ ì¤‘..."):
                    response = llm.invoke(formatted_prompt).content
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                with st.expander("ì°¸ê³ í•œ ê·œì • ì›ë¬¸"):
                    for i, doc in enumerate(final_context_docs):
                         title = doc.metadata.get("Article_Title", "ì¡°í•­/ë³„í‘œ")
                         source = doc.metadata.get("source", "íŒŒì¼")
                         st.markdown(f"**[ì°¸ê³  {i+1}: {source} - {title}]**")
                         st.text(doc.page_content[:200] + "...")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")