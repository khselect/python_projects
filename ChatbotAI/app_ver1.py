import streamlit as st
import tempfile
import os
import shutil
import re
import pymupdf4llm 
import time

# [ìƒˆë¡œ ì¶”ê°€ëœ Import] ChromaDB ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.
import chromadb 
from chromadb.errors import InternalError as ChromaInternalError

# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

# ğŸ‘‡ [í•„ìˆ˜ ìˆ˜ì •] ì„í¬íŠ¸ ê²½ë¡œ ë¶„ë¦¬ ë° PromptTemplate ì¶”ê°€
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

from langchain.docstore.document import Document
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import MarkdownHeaderTextSplitter

PERSIST_DIRECTORY = "./chroma_db"

st.set_page_config(page_title="ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI", layout="wide")
st.title("ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (í•œêµ­ì–´ ê°•ì œ & í‘œ ì¸ì‹ ê°•í™”)")

# --------------------------------------------------------------------------------
# 1. ë¬¸ì„œ ì „ì²˜ë¦¬ ë¡œì§ (ë°ì´í„° í´ë¦¬ë‹ ì¶”ê°€)
# --------------------------------------------------------------------------------
def clean_markdown_text(text):
    """
    LLMì´ ì˜¤í•´í•  ìˆ˜ ìˆëŠ” ë§ˆí¬ë‹¤ìš´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ì·¨ì†Œì„  ì œê±° í¬í•¨)
    """
    text = text.replace("~~", "") 
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text

def process_pdf_to_structured_docs(file_path, source_name):
    md_text = pymupdf4llm.to_markdown(file_path)
    md_text = clean_markdown_text(md_text)
    
    # êµ¬ì¡°í™” (ì œNì¡°, ë³„í‘œ ë“±ì„ í—¤ë”ë¡œ ë³€í™˜)
    md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\1# \2', md_text)
    md_text = re.sub(r'(^|\n)(\[ë³„í‘œ\s*\d+.*?\])', r'\1# \2', md_text)
    md_text = re.sub(r'(^|\n)(\[ë³„ì§€\s*.*?\])', r'\1# \2', md_text)

    # ë¶„í• 
    headers_to_split_on = [("#", "Article_Title")]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_header_splits = markdown_splitter.split_text(md_text)
    
    final_docs = []
    for doc in md_header_splits:
        doc.metadata["source"] = source_name
        final_docs.append(doc)
        
    return final_docs

# --------------------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” (ëª¨ë¸ ë° DB ì´ˆê¸°í™” ë¡œì§)
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    # [ìˆ˜ì •] ì»¤ìŠ¤í…€ ëª¨ë¸ ëª©ë¡
    CUSTOM_MODELS = ["korean-llama3", "korean-gemma2"] 
    selected_model = st.selectbox("AI ëª¨ë¸ ì„ íƒ", CUSTOM_MODELS, index=0)

    st.markdown("---")
    st.header("ğŸ“‚ ê·œì • ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("PDF ê·œì • íŒŒì¼", type=["pdf"], accept_multiple_files=True)
    process_button = st.button("ğŸš€ ê·œì • í•™ìŠµ ì‹œì‘")

    st.markdown("---")
    # DB ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì˜¤ë¥˜ ì‹œ í´ë¦­)"):
        st.session_state.clear() 
        try:
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY)
                st.success("âœ… DB ì‚­ì œ ì™„ë£Œ! F5ë¥¼ ëˆŒëŸ¬ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
            else:
                st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âš ï¸ íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. í„°ë¯¸ë„ì„ ê»ë‹¤ ì¼œê±°ë‚˜, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\nì—ëŸ¬: {e}")

# --------------------------------------------------------------------------------
# 3. ì„ë² ë”© ë° DB ì²˜ë¦¬
# --------------------------------------------------------------------------------
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

if uploaded_files and process_button:
    with st.spinner("ê·œì •ì˜ í‘œì™€ ì¡°í•­ì„ êµ¬ì¡°í™”í•˜ì—¬ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        for file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(file.getvalue())
                tmp_path = tmp.name
            
            try:
                docs = process_pdf_to_structured_docs(tmp_path, file.name)
                all_docs.extend(docs)
            finally:
                os.remove(tmp_path)
        
        if all_docs:
            vectorstore = Chroma(
                persist_directory=PERSIST_DIRECTORY,
                embedding_function=get_embeddings()
            )
            vectorstore.add_documents(all_docs)
            st.success(f"âœ… ì´ {len(all_docs)}ê°œì˜ ì¡°í•­/ë³„í‘œê°€ ì™„ë²½í•˜ê²Œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤!")
            time.sleep(1)
            st.rerun()

# --------------------------------------------------------------------------------
# 4. ê²€ìƒ‰ ë° ë‹µë³€ ë¡œì§ (Indentation/Logic Fix)
# --------------------------------------------------------------------------------
embeddings = get_embeddings()
vectorstore = None
ensemble_retriever = None

# [í•µì‹¬ ìˆ˜ì • ë¶€ë¶„] IndentationError ë° ë¡œë“œ ë¡œì§ ìˆ˜ì •
if os.path.exists(PERSIST_DIRECTORY):
    try:
        # DB ë¡œë“œ ì‹œë„
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
        
        # 1. Vector Search (ì •ìƒ ë¡œë“œ ì‹œì—ë§Œ Retriever ìƒì„±)
        chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # 2. BM25 Search (ì•™ìƒë¸” Retriever êµ¬ì„±)
        doc_data = vectorstore.get()
        if doc_data['documents']:
            bm25_docs = [Document(page_content=t, metadata=m) for t, m in zip(doc_data['documents'], doc_data['metadatas'])]
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 5
            
            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, chroma_retriever],
                weights=[0.8, 0.2]
            )
        else:
            ensemble_retriever = chroma_retriever
            
    # DB ì ê¸ˆ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬
    except ChromaInternalError:
        st.error("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì ê¸ˆ ì˜¤ë¥˜: DB íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ì„œë²„ë¥¼ ì™„ì „íˆ ì¢…ë£Œ(Ctrl+C) í›„, './chroma_db' í´ë”ë¥¼ ì‚­ì œí•˜ê³  ì¬ì‹œì‘í•˜ì„¸ìš”.")
        ensemble_retriever = None
    except Exception as e:
        st.error(f"DB ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        ensemble_retriever = None
# DBê°€ ì—†ìœ¼ë©´ ensemble_retrieverëŠ” ì´ˆê¸°ê°’ Noneì„ ìœ ì§€í•¨

# ì±„íŒ… UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš” (ì˜ˆ: ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ì•Œë ¤ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        if ensemble_retriever:
            llm = ChatOllama(
                model=selected_model, 
                base_url="http://127.0.0.1:11434",
            )
            
            # [ìˆ˜ì •] ì•ˆì •ì ì¸ PromptTemplate ì‚¬ìš© (System Prompt í¬í•¨)
            template = """
            [System Instruction]
            ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ê³µê³µê¸°ê´€ì˜ ì‚¬ë‚´ ê·œì •ì„ ì•ˆë‚´í•˜ëŠ” AI ì±—ë´‡ì´ë‹¤.
            ì£¼ì–´ì§„ [ë¬¸ë§¥(Context)]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ë¼.

            [ì ˆëŒ€ ì¤€ìˆ˜ ì›ì¹™]
            1. **ë¬´ì¡°ê±´ í•œêµ­ì–´(Korean)ë¡œë§Œ ë‹µë³€í•´ë¼.** ì˜ì–´ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆë¼.
            2. [ë¬¸ë§¥]ì— í‘œ(Table) ë‚´ìš©ì´ ìˆë‹¤ë©´, ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ë¼.
            3. "ê·œì • ì œOOì¡°ì— ë”°ë¥´ë©´..." ì²˜ëŸ¼ ê·¼ê±°ë¥¼ ë°˜ë“œì‹œ ëª…ì‹œí•´ë¼.
            4. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ê·œì •ì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´ë¼.

            [ë¬¸ë§¥(Context)]:
            {context}

            [ì§ˆë¬¸(Question)]:
            {question}

            ë‹µë³€(Answer):
            """

            QA_CHAIN_PROMPT = PromptTemplate(
                input_variables=["context", "question"],
                template=template
            )

            # ì²´ì¸ ì‹¤í–‰
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=ensemble_retriever,
                chain_type="stuff",
                chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
            )
            
            try:
                with st.spinner("ê·œì • ë¶„ì„ ì¤‘..."):
                    response = qa_chain.run(prompt)
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                with st.expander("ì°¸ê³ í•œ ê·œì • ì›ë¬¸"):
                    docs = ensemble_retriever.invoke(prompt)
                    for i, doc in enumerate(docs[:3]):
                         title = doc.metadata.get("Article_Title", "ì¡°í•­/ë³„í‘œ")
                         st.markdown(f"**[ê·¼ê±° {i+1}: {title}]**")
                         st.text(doc.page_content[:300] + "...")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë¨¼ì € ê·œì • íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")