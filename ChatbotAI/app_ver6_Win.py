import streamlit as st
import tempfile
import os
import shutil
import re
import pymupdf4llm 
import time
import mammoth  # docx ë³€í™˜ìš©
import markdownify # html to markdown ìš©
import olefile # hwp ê¸°ì´ˆ ë¶„ì„ìš©
import sys

# ChromaDB ê´€ë ¨ ì„í¬íŠ¸
import chromadb
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

PERSIST_DIRECTORY = "./chroma_db"

st.set_page_config(page_title="ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (Win)", layout="wide")
st.title("ğŸ›¡ï¸ ì‚¬ë‚´ ê·œì • ë§ˆìŠ¤í„° AI (Windows Hybrid Ver.)")

# --------------------------------------------------------------------------------
# 0. í•µì‹¬ í•¨ìˆ˜ ë¯¸ë¦¬ ì •ì˜ (ìˆœì„œ ë³€ê²½ë¨: ì—¬ê¸°ê°€ ì¤‘ìš”!)
# --------------------------------------------------------------------------------
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

# --------------------------------------------------------------------------------
# 1. ë¬¸ì„œ ì „ì²˜ë¦¬ ë¡œì§
# --------------------------------------------------------------------------------
def clean_markdown_text(text):
    text = text.replace("~~", "") 
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text

def convert_docx_to_markdown(docx_path):
    with open(docx_path, "rb") as docx_file:
        result = mammoth.convert_to_html(docx_file)
        html = result.value
    md_text = markdownify.markdownify(html, heading_style="ATX")
    return md_text

def extract_hwp_text(hwp_path):
    try:
        f = olefile.OleFileIO(hwp_path)
        encoded_text = f.openstream("PrvText").read()
        decoded_text = encoded_text.decode("utf-16le")
        return decoded_text
    except Exception as e:
        return f"[HWP ì˜¤ë¥˜] Word(.docx) ë³€í™˜ ê¶Œì¥. ë‚´ìš©: {e}"

def process_file_to_docs(file, source_name):
    file_ext = os.path.splitext(file.name)[1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(file.getvalue())
        tmp_path = tmp.name

    try:
        if file_ext == ".pdf":
            # pymupdf_layout ê²½ê³ ëŠ” ë¬´ì‹œí•´ë„ ë©ë‹ˆë‹¤ (ë‹¨ìˆœ ì•ˆë‚´ ë©”ì‹œì§€)
            md_text = pymupdf4llm.to_markdown(tmp_path)
        elif file_ext == ".docx":
            md_text = convert_docx_to_markdown(tmp_path)
        elif file_ext in [".hwp", ".hwpx"]:
            raw_text = extract_hwp_text(tmp_path)
            md_text = f"# {source_name} ë³¸ë¬¸\n\n{clean_markdown_text(raw_text)}"
        else:
            return []

        md_text = clean_markdown_text(md_text)
        
        # í—¤ë” ë³´ì •
        md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\1# \2', md_text)
        md_text = re.sub(r'(^|\n)(\[ë³„í‘œ\s*\d+.*?\])', r'\1# \2', md_text)
        md_text = re.sub(r'(^|\n)(\[ë³„ì§€\s*.*?\])', r'\1# \2', md_text)

        # 1ë‹¨ê³„ ì²­í‚¹
        headers_to_split_on = [("#", "Article_Title")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        header_splits = markdown_splitter.split_text(md_text)
        
        # 2ë‹¨ê³„ ì²­í‚¹
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        
        final_docs = []
        for doc in header_splits:
            splits = text_splitter.split_text(doc.page_content)
            for split_content in splits:
                new_doc = Document(
                    page_content=split_content,
                    metadata={
                        "source": source_name,
                        "Article_Title": doc.metadata.get("Article_Title", "ì¼ë°˜"),
                        "file_type": file_ext
                    }
                )
                final_docs.append(new_doc)
        return final_docs
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# --------------------------------------------------------------------------------
# 2. ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° & ì‚¬ì´ë“œë°”
# --------------------------------------------------------------------------------

# [ìˆ˜ì •ë¨] ì•± ì‹œì‘ ì‹œ ì €ì¥ëœ íŒŒì¼ëª… ë³µêµ¬ ë¡œì§ (get_embeddingsê°€ ì •ì˜ëœ í›„ ì‹¤í–‰ë¨)
if "learned_files" not in st.session_state:
    st.session_state.learned_files = []
    
    if os.path.exists(PERSIST_DIRECTORY):
        try:
            # ì—¬ê¸°ì„œ get_embeddings()ë¥¼ í˜¸ì¶œí•´ë„ ì´ì œ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            temp_db = Chroma(
                persist_directory=PERSIST_DIRECTORY, 
                embedding_function=get_embeddings()
            )
            
            existing_data = temp_db.get()
            if existing_data and existing_data['metadatas']:
                saved_files = set()
                for meta in existing_data['metadatas']:
                    if meta.get('source'):
                        saved_files.add(meta['source'])
                
                st.session_state.learned_files = list(saved_files)
                
        except Exception as e:
            # pymupdf ê²½ê³  ë“± ì¡ë‹¤í•œ ì˜¤ë¥˜ëŠ” ì½˜ì†”ì—ë§Œ ì°ê³  ë„˜ì–´ê°
            print(f"DB ë¡œë“œ ì¤‘ ê²½ë¯¸í•œ ì•Œë¦¼: {e}")

with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    CUSTOM_MODELS = ["korean-llama3", "korean-gemma2"] 
    selected_model = st.selectbox("AI ëª¨ë¸ ì„ íƒ", CUSTOM_MODELS, index=0)
    st.markdown("---")
    
    # í•™ìŠµëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    if st.session_state.learned_files:
        st.write("ğŸ“š **í•™ìŠµëœ ê·œì • ëª©ë¡:**")
        for f in st.session_state.learned_files:
            st.success(f"ğŸ“„ {f}")
    else:
        st.info("ì•„ì§ í•™ìŠµëœ ê·œì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    uploaded_files = st.file_uploader("ê·œì • íŒŒì¼ ì¶”ê°€ (PDF, DOCX)", type=["pdf", "docx", "hwp"], accept_multiple_files=True)
    process_button = st.button("ğŸš€ ê·œì • í•™ìŠµ ì‹œì‘")
    
    st.markdown("---")

    if st.button("ğŸ—‘ï¸ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™”"):
        st.session_state.clear()
        try:
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY)
                st.session_state.learned_files = []
                st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ. F5ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

# --------------------------------------------------------------------------------
# 3. í•™ìŠµ ì‹¤í–‰ ë¡œì§
# --------------------------------------------------------------------------------
if uploaded_files and process_button:
    with st.spinner("í•™ìŠµ ì¤‘..."):
        all_docs = []
        for file in uploaded_files:
            try:
                docs = process_file_to_docs(file, file.name)
                all_docs.extend(docs)
            except Exception as e:
                st.error(f"{file.name} ì˜¤ë¥˜: {e}")
        
        if all_docs:
            vectorstore = Chroma(
                persist_directory=PERSIST_DIRECTORY,
                embedding_function=get_embeddings()
            )
            vectorstore.add_documents(all_docs)
            st.success(f"âœ… í•™ìŠµ ì™„ë£Œ ({len(all_docs)} ì²­í¬)")
            
            for file in uploaded_files:
                if file.name not in st.session_state.learned_files:
                    st.session_state.learned_files.append(file.name)
            time.sleep(1)
            st.rerun()

# --------------------------------------------------------------------------------
# 4. ê²€ìƒ‰ ë° ë‹µë³€ ë¡œì§
# --------------------------------------------------------------------------------
chroma_retriever = None
bm25_retriever = None

if os.path.exists(PERSIST_DIRECTORY):
    try:
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=get_embeddings()
        )

        chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

        doc_data = vectorstore.get()
        if doc_data.get("documents"):
            bm25_docs = [
                Document(page_content=t, metadata=m)
                for t, m in zip(doc_data["documents"], doc_data["metadatas"])
            ]
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 10
            
            # ì—¬ê¸°ì„œ ì„±ê³µ ë©”ì‹œì§€ëŠ” ë„ˆë¬´ ìì£¼ ëœ¨ë©´ ê·€ì°®ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ printë¡œ ë³€ê²½ ê°€ëŠ¥
            print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ")
        else:
            st.info("â„¹ï¸ ë¬¸ì„œ ìˆ˜ê°€ ì ì–´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"âŒ DB ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì±„íŒ… UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        if chroma_retriever:
            llm = ChatOllama(
                model=selected_model, 
                base_url="http://127.0.0.1:11434",
                temperature=0,
                top_p=0.1
            )
            
            try:
                # ê²€ìƒ‰ ì‹¤í–‰
                vector_docs = chroma_retriever.invoke(prompt) if chroma_retriever else []
                bm25_docs = bm25_retriever.invoke(prompt) if bm25_retriever else []

                combined = vector_docs + bm25_docs
                
                unique_docs = []
                seen = set()
                for doc in combined:
                    key = doc.page_content[:150]
                    if key not in seen:
                        unique_docs.append(doc)
                        seen.add(key)

                final_context_docs = unique_docs[:5]
                context_text = "\n\n".join([doc.page_content for doc in final_context_docs])

                template = """
                [System Instruction]
                ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì • ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ [Context]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
                ì‚¬ìš©ìê°€ ì½ê¸° í¸í•˜ê²Œ ë°”ë¡œ ê²°ë¡ ë¶€í„° ë‹µë³€í•˜ì„¸ìš”. 
                "ì œì•½ ì¡°ê±´ì„ ì¤€ìˆ˜í–ˆìŠµë‹ˆë‹¤" ê°™ì€ ë¶ˆí•„ìš”í•œ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.

                [Context]:
                {context}

                [Question]:
                {question}

                ë‹µë³€(í•œêµ­ì–´):
                """
                
                prompt_obj = PromptTemplate(
                    input_variables=["context", "question"],
                    template=template
                )
                formatted_prompt = prompt_obj.format(context=context_text, question=prompt)
                
                with st.spinner("ë¶„ì„ ì¤‘..."):
                    response = llm.invoke(formatted_prompt).content
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                with st.expander("ì°¸ê³ í•œ ê·œì • ì›ë¬¸"):
                    for i, doc in enumerate(final_context_docs):
                         title = doc.metadata.get("Article_Title", "ì¡°í•­/ë³„í‘œ")
                         source = doc.metadata.get("source", "íŒŒì¼")
                         st.markdown(f"**[ì°¸ê³  {i+1}: {source} - {title}]**")
                         st.text(doc.page_content[:200] + "...")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
        else:
            st.warning("ê·œì •ì„ ë¨¼ì € í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")